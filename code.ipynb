{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE8hHX-5hwYj",
        "outputId": "5da0e01b-70b7-4033-d0bf-470f8b0a69c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n",
            "Filtered data has been saved to 'filtered_license_context_only.csv'\n",
            "Filtered data has been saved to 'filtered_llm_applications_context_only.csv'\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "!pip install pdfminer.six\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Load the PDF file\n",
        "def load_pdf(file_path):\n",
        "    try:\n",
        "        document = fitz.open(file_path)\n",
        "        return document\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Extract all text from the PDF using MuPDF or fallback to PDFMiner if MuPDF fails\n",
        "def extract_text_from_pdf(document, file_path):\n",
        "    if document:\n",
        "        text = \"\"\n",
        "        try:\n",
        "            for page_num in range(len(document)):\n",
        "                page = document.load_page(page_num)\n",
        "                text += f\"PAGE {page_num + 1}\\n\" + page.get_text()\n",
        "            return text\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Fallback to using PDFMiner\n",
        "    try:\n",
        "        return extract_text(file_path)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# Function to check if a page contains references\n",
        "def is_reference_page(page_text):\n",
        "    reference_keywords = [\"references\", \"bibliography\", \"references and notes\",\"CrossRef\"]\n",
        "    for keyword in reference_keywords:\n",
        "        if re.search(r'\\b' + keyword + r'\\b', page_text, re.IGNORECASE):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Simplified search function using a list of search terms\n",
        "def search_sentences_with_context(text, search_list):\n",
        "    \"\"\"\n",
        "    Search for sentences in the text that contain any of the terms in the search list,\n",
        "    and extract the sentence along with 10 preceding and 10 following sentences.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    # Split the text by pages\n",
        "    pages = text.split(\"PAGE \")\n",
        "    for page in pages[1:]:  # Skip the first split since it will be empty\n",
        "        page_number, page_text = page.split(\"\\n\", 1)\n",
        "\n",
        "        # Skip the page if it is a reference page\n",
        "        if is_reference_page(page_text):\n",
        "            continue\n",
        "\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', page_text)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            for term in search_list:\n",
        "                pattern = re.compile(rf'\\b{re.escape(term)}\\b', re.IGNORECASE)\n",
        "                if pattern.search(sentence):\n",
        "                    # Extract context: 10 sentences before and after the matched sentence\n",
        "                    context = \" \".join(sentences[max(0, i - 10):min(len(sentences), i + 11)])\n",
        "\n",
        "                    # Find the line number\n",
        "                    line_number = page_text[:page_text.find(sentence)].count('\\n') + 1\n",
        "                    result.append((term, sentence, context, int(page_number), line_number))\n",
        "    return result\n",
        "\n",
        "# Process multiple PDF documents\n",
        "def process_pdfs(file_paths, search_list):\n",
        "    all_results = []\n",
        "    for file_path in file_paths:\n",
        "        document = load_pdf(file_path)\n",
        "        text = extract_text_from_pdf(document, file_path) if document else extract_text(file_path)\n",
        "        results = search_sentences_with_context(text, search_list)\n",
        "        search_items = \"; \".join(search_list)\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        if results:\n",
        "            # Extract the title from the first page or first few lines of the text\n",
        "            title = text.splitlines()[0] if text.splitlines() else \"Unknown Title\"\n",
        "            for item in results:\n",
        "                all_results.append((file_name, title, item[1], f\"found by {item[0]}\", item[3], item[4], item[2]))\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# Save results to CSV file\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "        writer.writerow([\"File\", \"Title\", \"Sentence\", \"Found In\", \"Page\", \"Line\", \"Context\"])\n",
        "        for result in results:\n",
        "            writer.writerow([str(field) for field in result])\n",
        "\n",
        "# Function to find licenses in text and return the matched license\n",
        "def find_license_in_text(text, license_list):\n",
        "    for license in license_list:\n",
        "        if license in text:\n",
        "            return license\n",
        "    return None\n",
        "\n",
        "# Function to find LLM applications in text and return the matched application\n",
        "def find_application_in_text(text, application_list):\n",
        "    for application in application_list:\n",
        "        if application in text:\n",
        "            return application\n",
        "    return None\n",
        "\n",
        "# Process PDFs and save results to a CSV file\n",
        "def main():\n",
        "    current_dir = os.getcwd()\n",
        "    file_paths = [os.path.join(current_dir, f) for f in os.listdir(current_dir) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    # LLM list to search for\n",
        "    llm_list = [\n",
        "        'GPT-3', 'GPT-4', 'BERT', 'RoBERTa', 'XLNet', 'T5', 'ALBERT',\n",
        "        'DistilBERT', 'ERNIE', 'ELECTRA', 'Megatron', 'Turing-NLG',\n",
        "        'Transformer-XL', 'OpenAI Codex', 'BART', 'Pegasus', 'Reformer',\n",
        "        'CTRL', 'BigGAN', 'DeBERTa', 'BlenderBot', 'GShard', 'Switch-Transformer',\n",
        "        'PanGu-α', 'MT-NLG', 'GLaM', 'WuDao', 'Jurassic-1', 'LaMDA', 'OPT',\n",
        "        'LLaMA', 'ChatGPT',\n",
        "        'GPT-2', 'WebGPT', 'Ernie 3.0 Titan', 'Gopher', 'FLAN', 'Yuan 1.0',\n",
        "        'T0', 'HyperCLOVA', 'NAVER', 'CPM-2', 'PLUG', 'CodeGen', 'Chinchilla',\n",
        "        'AlphaCode', 'GPT-NeoX-20B', 'Tk-Instruct', 'Cohere', 'UL2',\n",
        "        'PaLM', 'YaLM', 'CodeGeeX', 'GLM', 'AlexaTM', 'WeLM', 'RWKV',\n",
        "        'Sparrow', 'Flan-T5', 'Flan-PaLM', 'Luminous', 'NLLB', 'BLOOM',\n",
        "        'mT0', 'BLOOMZ', 'Galatica', 'OPT-IML', 'Gemini', 'Ernie 4.0',\n",
        "        'Grok-1', 'Aquila2', 'FLM', 'QWEN', 'Baichuan2', 'LLaMA2',\n",
        "        'Falcon 180B', 'CodeGen2', 'PaLM2', 'Baichuan', 'MPT',\n",
        "        'InternLM', 'LLaMA 65B', 'Bard', 'PanGu-sum', 'Vicuna',\n",
        "        'Pythia', 'Sora', 'Gemini 1.5', 'Claude 3 Opus',\n",
        "        'Llama3', 'phi-3',\n",
        "        'Mistral', 'Claude', 'Claude 2', 'Claude 3', 'Claude Next',\n",
        "        'Anthropic LLM', 'ChatGPT-4 Turbo', 'Megatron-Turing NLG',\n",
        "        'GPT-J', 'GPT-Neo', 'Bloom', 'Jurrasic-2', 'BLOOMZ',\n",
        "        'Alpaca', 'StableLM', 'Phoenix', 'XGen', 'Grok-2',\n",
        "        'Gemini 1', 'Gemini 2', 'Gemini 3', 'Aquila', 'Aquila-Chat',\n",
        "        'Meta AI BlenderBot', 'Sirius', 'Prometheus', 'Orca',\n",
        "        'Falcon', 'BLOOM 176B', 'Turing-NLG 17B', 'Megatron-LM',\n",
        "        'T5-11B', 'BigScience', 'Coqui TTS', 'Neuralink LM',\n",
        "        'Mosaic GPT', 'Mosaic BERT', 'Turing', 'Unity-3B', 'Hera'\n",
        "    ]\n",
        "\n",
        "    # Process PDFs for LLM list and save results to a CSV file\n",
        "    results_llms = process_pdfs(file_paths, llm_list)\n",
        "    save_results_to_csv(results_llms, 'search_results_llms.csv')\n",
        "\n",
        "    # License list to search for\n",
        "    search_list_licenses = [\n",
        "        'AFL-3.0', 'Academic Free License v3.0', 'Apache-2.0',\n",
        "        'Apache license 2.0', 'Artistic-2.0', 'Artistic license 2.0',\n",
        "        'BSL-1.0', 'Boost Software License 1.0', 'BSD-2-Clause',\n",
        "        \"BSD 2-clause 'Simplified' license\", 'BSD-3-Clause',\n",
        "        \"BSD 3-clause 'New' or 'Revised' license\", 'BSD-3-Clause-Clear',\n",
        "        'BSD 3-clause Clear license', 'BSD-4-Clause',\n",
        "        \"BSD 4-clause 'Original' or 'Old' license\", '0BSD',\n",
        "        'BSD Zero-Clause license', 'CC0-1.0', 'Creative Commons Zero v1.0 Universal',\n",
        "        'CC-BY-4.0', 'Creative Commons Attribution 4.0',\n",
        "        'CC-BY-SA-4.0', 'Creative Commons Attribution ShareAlike 4.0',\n",
        "        'ECL-2.0','Educational Community License v2.0', 'EPL-1.0',\n",
        "        'Eclipse Public License 1.0',\n",
        "        'EPL-2.0', 'Eclipse Public License 2.0', 'EUPL-1.1',\n",
        "        'European Union Public License 1.1', 'AGPL-3.0',\n",
        "        'GNU Affero General Public License v3.0', 'GPL',\n",
        "        'GNU General Public License family', 'GPL-2.0',\n",
        "        'GNU General Public License v2.0', 'GPL-3.0', 'GNU General Public License v3.0',\n",
        "        'LGPL', 'GNU Lesser General Public License family', 'LGPL-2.1',\n",
        "        'GNU Lesser General Public License v2.1', 'LGPL-3.0',\n",
        "        'GNU Lesser General Public License v3.0',  'ISC', 'LPPL-1.3c',\n",
        "        'LaTeX Project Public License v1.3c', 'MS-PL', 'Microsoft Public License',\n",
        "        'MIT', 'MPL-2.0', 'Mozilla Public License 2.0', 'OSL-3.0',\n",
        "        'Open Software License 3.0', 'PostgreSQL', 'PostgreSQL License',\n",
        "        'OFL-1.1', 'SIL Open Font License 1.1', 'NCSA',\n",
        "        'University of Illinois/NCSA Open Source License',\n",
        "        'Unlicense', 'The Unlicense', 'Zlib', 'zLib License', 'license', 'licence'\n",
        "    ]\n",
        "\n",
        "    # Create a DataFrame to store the results\n",
        "    df = pd.read_csv('search_results_llms.csv')\n",
        "    filtered_df = pd.DataFrame(columns=['File', 'Title', 'Sentence', 'Context', 'Page', 'Line', 'Found License'])\n",
        "\n",
        "    # Check each row for license mentions\n",
        "    for index, row in df.iterrows():\n",
        "        found_license_in_context = find_license_in_text(row['Context'], search_list_licenses)\n",
        "        if found_license_in_context:\n",
        "            new_row = row.copy()\n",
        "            new_row['Found License'] = found_license_in_context\n",
        "            new_row['Context'] = \" \".join(new_row['Context'].split())\n",
        "            filtered_df = pd.concat([filtered_df, pd.DataFrame([new_row])])\n",
        "\n",
        "    # Remove duplicates and save to CSV\n",
        "    filtered_df = filtered_df.drop_duplicates(subset=['Context'])\n",
        "    filtered_df.to_csv('filtered_license_context_only.csv', index=False)\n",
        "    print(\"Filtered data has been saved to 'filtered_license_context_only.csv'\")\n",
        "\n",
        "    # LLM applications to search for\n",
        "    llm_applications = [\n",
        "        'Text Generation and Auto-Writing',\n",
        "        'Machine Translation',\n",
        "        'Question Answering Systems',\n",
        "        'Text Summarization',\n",
        "        'Sentiment Analysis',\n",
        "        'Conversational Agents',\n",
        "        'Information Retrieval',\n",
        "        'Content Creation Assistance',\n",
        "        'Code Generation and Programming Assistance',\n",
        "        'Personalized Recommendations',\n",
        "        'Grammar Checking and Text Proofreading',\n",
        "        'Knowledge Graph Construction',\n",
        "        'Text Classification',\n",
        "        'Fine-Tuning for Domain-Specific Tasks',\n",
        "        'Speech Recognition and Synthesis',\n",
        "        'Virtual Assistants',\n",
        "        'Automatic Alignment and Synchronization',\n",
        "        'Education and Learning Support',\n",
        "        'Data Augmentation',\n",
        "        'Knowledge Exploration and Inquiry'\n",
        "    ]\n",
        "\n",
        "    # Create a DataFrame to store the results\n",
        "    filtered_df = pd.DataFrame(columns=['File', 'Title', 'Sentence', 'Context', 'Page', 'Line', 'Found Application'])\n",
        "\n",
        "    # Check each row for LLM application mentions\n",
        "    for index, row in df.iterrows():\n",
        "        found_application_in_context = find_application_in_text(row['Context'], llm_applications)\n",
        "        if found_application_in_context:\n",
        "            new_row = row.copy()\n",
        "            new_row['Found Application'] = found_application_in_context\n",
        "            new_row['Context'] = \" \".join(new_row['Context'].split())\n",
        "            filtered_df = pd.concat([filtered_df, pd.DataFrame([new_row])])\n",
        "\n",
        "    # Remove duplicates and save to CSV\n",
        "    filtered_df = filtered_df.drop_duplicates(subset=['Context'])\n",
        "    filtered_df.to_csv('filtered_llm_applications_context_only.csv', index=False)\n",
        "    print(\"Filtered data has been saved to 'filtered_llm_applications_context_only.csv'\")\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Define the list of model names and their applications/licenses\n",
        "llm_list = [\n",
        "    'GPT-3', 'GPT-4', 'BERT', 'RoBERTa', 'XLNet', 'T5', 'ALBERT',\n",
        "    'DistilBERT', 'ERNIE', 'ELECTRA', 'Megatron', 'Turing-NLG',\n",
        "    'Transformer-XL', 'OpenAI Codex', 'BART', 'Pegasus', 'Reformer',\n",
        "    'CTRL', 'BigGAN', 'DeBERTa', 'BlenderBot', 'GShard', 'Switch-Transformer',\n",
        "    'PanGu-α', 'MT-NLG', 'GLaM', 'WuDao', 'Jurassic-1', 'LaMDA', 'OPT',\n",
        "    'LLaMA', 'ChatGPT', 'GPT-2', 'WebGPT', 'Ernie 3.0 Titan', 'Gopher', 'FLAN',\n",
        "    'Yuan 1.0', 'T0', 'HyperCLOVANAVER', 'CPM-2', 'PLUG', 'CodeGen', 'Chinchilla',\n",
        "    'AlphaCode', 'GPT-NeoX-20B', 'Tk-Instruct', 'Cohere', 'UL2', 'PaLM', 'YaLM',\n",
        "    'CodeGeeX', 'GLM', 'AlexaTM', 'WeLM', 'RWKV', 'Sparrow', 'Flan-T5', 'Flan-PaLM',\n",
        "    'Luminous', 'NLLB', 'BLOOM', 'mT0', 'BLOOMZ', 'Galatica', 'OPT-IML', 'Gemini',\n",
        "    'Ernie 4.0', 'Grok-1', 'Aquila2', 'FLM', 'QWEN', 'Baichuan2', 'LLaMA2',\n",
        "    'Falcon 180B', 'CodeGen2', 'PaLM2', 'Baichuan', 'MPT', 'InternLM', 'LLaMA 65B',\n",
        "    'Bard', 'PanGu-sum', 'Vicuna', 'Pythia', 'Sora', 'Gemini 1.5', 'Claude 3 Opus',\n",
        "    'Llama3', 'phi-3', 'Mistral', 'Claude', 'Claude 2', 'Claude 3', 'Claude Next',\n",
        "    'Anthropic LLM', 'ChatGPT-4 Turbo', 'Megatron-Turing NLG', 'GPT-J', 'GPT-Neo',\n",
        "    'Bloom', 'Jurrasic-2', 'BLOOMZ', 'Alpaca', 'StableLM', 'Phoenix', 'XGen',\n",
        "    'Grok-2', 'Gemini 1', 'Gemini 2', 'Gemini 3', 'Aquila', 'Aquila-Chat',\n",
        "    'Meta AI BlenderBot', 'Sirius', 'Prometheus', 'Orca', 'Falcon', 'BLOOM 176B',\n",
        "    'Turing-NLG 17B', 'Megatron-LM', 'T5-11B', 'BigScience', 'Coqui TTS',\n",
        "    'Neuralink LM', 'Mosaic GPT', 'Mosaic BERT', 'Turing', 'Unity-3B', 'Hera'\n",
        "]\n",
        "\n",
        "# Define list of possible LLM applications\n",
        "llm_applications = [\n",
        "    'Text Generation and Auto-Writing', 'Machine Translation',\n",
        "    'Question Answering Systems', 'Text Summarization', 'Sentiment Analysis',\n",
        "    'Conversational Agents', 'Information Retrieval', 'Content Creation Assistance',\n",
        "    'Code Generation and Programming Assistance', 'Personalized Recommendations',\n",
        "    'Grammar Checking and Text Proofreading', 'Knowledge Graph Construction',\n",
        "    'Text Classification', 'Fine-Tuning for Domain-Specific Tasks',\n",
        "    'Speech Recognition and Synthesis', 'Virtual Assistants',\n",
        "    'Automatic Alignment and Synchronization', 'Education and Learning Support',\n",
        "    'Data Augmentation', 'Knowledge Exploration and Inquiry'\n",
        "]\n",
        "\n",
        "# Define list of possible licenses\n",
        "search_list_licenses = [\n",
        "    'AFL-3.0', 'Academic Free License v3.0', 'Apache-2.0', 'Apache license 2.0',\n",
        "    'Artistic-2.0', 'Artistic license 2.0', 'BSL-1.0', 'Boost Software License 1.0',\n",
        "    'BSD-2-Clause', \"BSD 2-clause 'Simplified' license\", 'BSD-3-Clause',\n",
        "    \"BSD 3-clause 'New' or 'Revised' license\", 'BSD-3-Clause-Clear',\n",
        "    'BSD 3-clause Clear license', 'BSD-4-Clause', \"BSD 4-clause 'Original' or 'Old' license\",\n",
        "    '0BSD', 'BSD Zero-Clause license', 'CC0-1.0', 'Creative Commons Zero v1.0 Universal',\n",
        "    'CC-BY-4.0', 'Creative Commons Attribution 4.0', 'Creative Commons Attribution International 4.0',\n",
        "    'CC-BY-SA-4.0', 'Creative Commons Attribution ShareAlike 4.0', 'ECL-2.0',\n",
        "    'Educational Community License v2.0', 'EPL-1.0', 'Eclipse Public License 1.0',\n",
        "    'EPL-2.0', 'Eclipse Public License 2.0', 'EUPL-1.1', 'European Union Public License 1.1',\n",
        "    'AGPL-3.0', 'GNU Affero General Public License v3.0', 'GPL', 'GNU General Public License family',\n",
        "    'GPL-2.0', 'GNU General Public License v2.0', 'GPL-3.0', 'GNU General Public License v3.0',\n",
        "    'LGPL', 'GNU Lesser General Public License family', 'LGPL-2.1', 'GNU Lesser General Public License v2.1',\n",
        "    'LGPL-3.0', 'GNU Lesser General Public License v3.0', 'ISC', 'LPPL-1.3c',\n",
        "    'LaTeX Project Public License v1.3c', 'MS-PL', 'Microsoft Public License',\n",
        "    'MIT', 'MPL-2.0', 'Mozilla Public License 2.0', 'OSL-3.0',\n",
        "    'Open Software License 3.0', 'PostgreSQL', 'PostgreSQL License',\n",
        "    'OFL-1.1', 'SIL Open Font License 1.1', 'NCSA', 'University of Illinois/NCSA Open Source License',\n",
        "    'Unlicense', 'The Unlicense', 'Zlib', 'zLib License', 'license', 'licence'\n",
        "]\n",
        "\n",
        "# Function to label tokens in a context with BIO tags for models, applications, and licenses\n",
        "def label_tokens(context, llm_list, application_list, license_list):\n",
        "    tokens = context.split()\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    # Label models\n",
        "    for model in llm_list:\n",
        "        match = re.search(re.escape(model), context)\n",
        "        if match:\n",
        "            start_idx = len(context[:match.start()].split())\n",
        "            end_idx = len(context[:match.end()].split()) - 1\n",
        "            labels[start_idx] = \"B-MODEL\"\n",
        "            for i in range(start_idx + 1, end_idx + 1):\n",
        "                labels[i] = \"I-MODEL\"\n",
        "\n",
        "    # Label applications\n",
        "    for application in application_list:\n",
        "        match = re.search(re.escape(application), context)\n",
        "        if match:\n",
        "            start_idx = len(context[:match.start()].split())\n",
        "            end_idx = len(context[:match.end()].split()) - 1\n",
        "            labels[start_idx] = \"B-APPLICATION\"\n",
        "            for i in range(start_idx + 1, end_idx + 1):\n",
        "                labels[i] = \"I-APPLICATION\"\n",
        "\n",
        "    # Label licenses\n",
        "    for license in license_list:\n",
        "        match = re.search(re.escape(license), context)\n",
        "        if match:\n",
        "            start_idx = len(context[:match.start()].split())\n",
        "            end_idx = len(context[:match.end()].split()) - 1\n",
        "            labels[start_idx] = \"B-LICENSE\"\n",
        "            for i in range(start_idx + 1, end_idx + 1):\n",
        "                labels[i] = \"I-LICENSE\"\n",
        "\n",
        "    return tokens, labels\n",
        "\n",
        "# Load the CSV files\n",
        "applications_file_path = '/content/sample_data/filtered_llm_applications_context_only.csv'  # Replace with your file path\n",
        "licenses_file_path = '/content/sample_data/filtered_license_context_only.csv'  # Replace with your file path\n",
        "\n",
        "# Process and label contexts in the applications file\n",
        "df_applications = pd.read_csv(applications_file_path)\n",
        "labeled_data_applications = []\n",
        "for index, row in df_applications.iterrows():\n",
        "    tokens, labels = label_tokens(row['Context'], llm_list, llm_applications, [])\n",
        "    labeled_data_applications.append({\"Context\": row['Context'], \"Tokens\": tokens, \"Labels\": labels})\n",
        "\n",
        "# Convert the labeled application data to a DataFrame and save it\n",
        "labeled_df_applications = pd.DataFrame(labeled_data_applications)\n",
        "labeled_df_applications.to_csv('labeled_contexts_with_bio_applications.csv', index=False)\n",
        "print(\"Labeled data for applications has been saved to 'labeled_contexts_with_bio_applications.csv'\")\n",
        "\n",
        "# Process and label contexts in the licenses file\n",
        "df_licenses = pd.read_csv(licenses_file_path)\n",
        "labeled_data_licenses = []\n",
        "for index, row in df_licenses.iterrows():\n",
        "    tokens, labels = label_tokens(row['Context'], llm_list, [], search_list_licenses)\n",
        "    labeled_data_licenses.append({\"Context\": row['Context'], \"Tokens\": tokens, \"Labels\": labels})\n",
        "\n",
        "# Convert the labeled license data to a DataFrame and save it\n",
        "labeled_df_licenses = pd.DataFrame(labeled_data_licenses)\n",
        "labeled_df_licenses.to_csv('labeled_contexts_with_bio.csv', index=False)\n",
        "print(\"Labeled data for licenses has been saved to 'labeled_contexts_with_bio.csv'\")\n"
      ],
      "metadata": {
        "id": "0b9XrE7PL7TX",
        "outputId": "0d01a131-139c-46b9-834d-8a53846d6057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeled data for applications has been saved to 'labeled_contexts_with_bio_applications.csv'\n",
            "Labeled data for licenses has been saved to 'labeled_contexts_with_bio.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cairosvg\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import cairosvg\n",
        "\n",
        "# Load the pre-trained spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process a sentence with the spaCy model\n",
        "doc = nlp(\"ERNIE is effective in Named Entity Recognition (NER), particularly in Chinese text.\")\n",
        "\n",
        "# Output the dependency relations for each token in the sentence\n",
        "for token in doc:\n",
        "    print(token.text + '\\t' + token.lemma_ + '\\t' + token.pos_ + '\\t' + token.tag_ + '\\t' + token.dep_ + '\\t' + str([child.text + ':' + child.dep_ for child in token.children]))\n",
        "\n",
        "# Render the dependency parse tree using spaCy's displaCy visualizer and save it as an SVG\n",
        "svg = displacy.render(doc, style='dep', jupyter=False, options={'compact': True})\n",
        "\n",
        "# Convert the SVG data to PNG format using cairosvg\n",
        "png_data = cairosvg.svg2png(bytestring=svg.encode('utf-8'))\n",
        "\n",
        "# Load the PNG data into a PIL Image object\n",
        "img = Image.open(BytesIO(png_data))\n",
        "\n",
        "# Save the image as a PNG file\n",
        "img.save(\"dependency_parse.png\")\n",
        "\n",
        "# Display the image using matplotlib\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # Hide the axes for a cleaner display\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "X5b5SRk8oB7Z",
        "outputId": "0f4b644d-580b-4db7-95d2-5d761ee6255e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cairosvg\n",
            "  Downloading CairoSVG-2.7.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting cairocffi (from cairosvg)\n",
            "  Downloading cairocffi-1.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting cssselect2 (from cairosvg)\n",
            "  Downloading cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from cairosvg) (0.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from cairosvg) (9.4.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from cairosvg) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from cairocffi->cairosvg) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.22)\n",
            "Downloading CairoSVG-2.7.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cairocffi-1.7.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: cssselect2, cairocffi, cairosvg\n",
            "Successfully installed cairocffi-1.7.1 cairosvg-2.7.1 cssselect2-0.7.0\n",
            "ERNIE\tERNIE\tPROPN\tNNP\tnsubj\t[]\n",
            "is\tbe\tAUX\tVBZ\tROOT\t['ERNIE:nsubj', 'effective:acomp', '.:punct']\n",
            "effective\teffective\tADJ\tJJ\tacomp\t['in:prep']\n",
            "in\tin\tADP\tIN\tprep\t['Recognition:pobj']\n",
            "Named\tname\tVERB\tVBN\tcompound\t[]\n",
            "Entity\tEntity\tPROPN\tNNP\tcompound\t[]\n",
            "Recognition\tRecognition\tPROPN\tNNP\tpobj\t['Named:compound', 'Entity:compound', '(:punct', 'NER:appos', '):punct', ',:punct', 'in:prep']\n",
            "(\t(\tPUNCT\t-LRB-\tpunct\t[]\n",
            "NER\tNER\tPROPN\tNNP\tappos\t[]\n",
            ")\t)\tPUNCT\t-RRB-\tpunct\t[]\n",
            ",\t,\tPUNCT\t,\tpunct\t[]\n",
            "particularly\tparticularly\tADV\tRB\tadvmod\t[]\n",
            "in\tin\tADP\tIN\tprep\t['particularly:advmod', 'text:pobj']\n",
            "Chinese\tchinese\tADJ\tJJ\tamod\t[]\n",
            "text\ttext\tNOUN\tNN\tpobj\t['Chinese:amod']\n",
            ".\t.\tPUNCT\t.\tpunct\t[]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the relationship extraction function\n",
        "def extract_relationship(doc):\n",
        "    subject = None\n",
        "    obj = None\n",
        "    relation = None\n",
        "    is_passive = False\n",
        "\n",
        "    # Check if the sentence is in passive voice\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"auxpass\":\n",
        "            is_passive = True\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"ROOT\":\n",
        "            # Extract the main verb along with any auxiliary verbs\n",
        "            aux_verbs = \" \".join([child.text for child in token.children if child.dep_ in (\"aux\", \"auxpass\")])\n",
        "            relation = f\"{aux_verbs} {token.text}\".strip()\n",
        "\n",
        "            if is_passive:\n",
        "                # Handle passive voice\n",
        "                for child in token.children:\n",
        "                    if child.dep_ == \"nsubjpass\":\n",
        "                        subject = \" \".join([t.text for t in child.subtree])  # Extract passive subject\n",
        "                    if child.dep_ == \"prep\":\n",
        "                        if obj is None:\n",
        "                            preposition = child.text\n",
        "                            for pobj in child.children:\n",
        "                                if pobj.dep_ == \"pobj\":\n",
        "                                    obj = \" \".join([t.text for t in pobj.subtree])\n",
        "                            relation = f\"{relation} {preposition}\".strip()\n",
        "            else:\n",
        "                # Handle active voice\n",
        "                for child in token.children:\n",
        "                    if child.dep_ == \"nsubj\":\n",
        "                        subject = \" \".join([t.text for t in child.subtree])  # Extract active subject\n",
        "                    if child.dep_ == \"acomp\":\n",
        "                        # Handle adjectival complements and prepositional phrases\n",
        "                        relation = f\"{relation} {child.text}\".strip()\n",
        "                        for acomp_child in child.children:\n",
        "                            if acomp_child.dep_ == \"prep\" and obj is None:\n",
        "                                preposition = acomp_child.text\n",
        "                                for pobj in acomp_child.children:\n",
        "                                    if pobj.dep_ == \"pobj\":\n",
        "                                        obj = \" \".join([t.text for t in pobj.subtree])\n",
        "                                relation = f\"{relation} {preposition}\".strip()\n",
        "                    elif child.dep_ == \"dobj\" and obj is None:\n",
        "                        obj = \" \".join([t.text for t in child.subtree])  # Extract direct object\n",
        "                    elif child.dep_ == \"prep\" and obj is None:\n",
        "                        # Handle prepositional phrases in active voice\n",
        "                        preposition = child.text\n",
        "                        for pobj in child.children:\n",
        "                            if pobj.dep_ == \"pobj\":\n",
        "                                obj = \" \".join([t.text for t in pobj.subtree])\n",
        "                        relation = f\"{relation} {preposition}\".strip()\n",
        "\n",
        "    if subject and obj and relation:\n",
        "        # Remove the article 'the' from the object if present\n",
        "        if obj.startswith(\"the \"):\n",
        "            obj = obj[4:]  # Remove \"the \"\n",
        "        return (subject, relation, obj)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Process sentences to extract relationships\n",
        "def process_sentences(file_path, sentence_column):\n",
        "    # Read the dataset\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    relations = []\n",
        "    for sentence in data[sentence_column]:\n",
        "        doc = nlp(sentence)\n",
        "        relationship = extract_relationship(doc)\n",
        "        if relationship:\n",
        "            relations.append(relationship)\n",
        "        else:\n",
        "            relations.append(('N/A', 'N/A', 'N/A'))  # If no relationship is found, mark as N/A\n",
        "\n",
        "    # Create a DataFrame for the extracted relationships\n",
        "    relations_df = pd.DataFrame(relations, columns=['Subject', 'Relation', 'Object'])\n",
        "\n",
        "    return relations_df\n",
        "\n",
        "# Define paths and columns for different datasets\n",
        "llm_file_path = '/content/sample_data/llm_model_applications.csv'\n",
        "license_file_path = '/content/sample_data/license_sentences.csv'\n",
        "\n",
        "# Process the LLM dataset\n",
        "llm_relations_df = process_sentences(llm_file_path, 'sentence')\n",
        "llm_output_path = 'extracted_llm_relationships.csv'\n",
        "llm_relations_df.to_csv(llm_output_path, index=False)\n",
        "\n",
        "# Process the license dataset\n",
        "license_relations_df = process_sentences(license_file_path, 'Sentence')\n",
        "license_output_path = 'extracted_license_relationships.csv'\n",
        "license_relations_df.to_csv(license_output_path, index=False)\n",
        "\n",
        "# Clean the extracted relationships\n",
        "def clean_extracted_data(file_path):\n",
        "    # Load the extracted relationships dataset\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Display original data for verification\n",
        "    print(\"Original Data:\")\n",
        "    print(data.head())\n",
        "\n",
        "    # Remove rows with N/A values\n",
        "    data_cleaned = data.dropna()\n",
        "\n",
        "    # Remove duplicate rows\n",
        "    data_cleaned = data_cleaned.drop_duplicates()\n",
        "\n",
        "    # Save the cleaned data\n",
        "    cleaned_output_path = 'cleaned_' + file_path\n",
        "    data_cleaned.to_csv(cleaned_output_path, index=False)\n",
        "\n",
        "    # Display cleaned data for verification\n",
        "    print(\"\\nCleaned Data:\")\n",
        "    print(data_cleaned.head())\n",
        "\n",
        "    # Return the path to the cleaned file\n",
        "    return cleaned_output_path\n",
        "\n",
        "# Clean both extracted datasets and save the cleaned versions\n",
        "cleaned_llm_path = clean_extracted_data(llm_output_path)\n",
        "cleaned_license_path = clean_extracted_data(license_output_path)\n",
        "\n",
        "# Display output paths for easy access\n",
        "print(f\"Cleaned LLM relationships saved to: {cleaned_llm_path}\")\n",
        "print(f\"Cleaned License relationships saved to: {cleaned_license_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrU866nMoaYl",
        "outputId": "1c4ecbb9-ac78-416b-fcd1-87c1751173d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "  Subject     Relation                 Object\n",
            "0   GPT-3  is used for        Text Generation\n",
            "1   GPT-3       powers  Conversational Agents\n",
            "2     NaN          NaN                    NaN\n",
            "3   GPT-4     enhances        Text Generation\n",
            "4   GPT-4     improves     Question Answering\n",
            "\n",
            "Cleaned Data:\n",
            "  Subject     Relation                 Object\n",
            "0   GPT-3  is used for        Text Generation\n",
            "1   GPT-3       powers  Conversational Agents\n",
            "3   GPT-4     enhances        Text Generation\n",
            "4   GPT-4     improves     Question Answering\n",
            "5   GPT-4   is used in       Creative Writing\n",
            "Original Data:\n",
            "   Subject              Relation                             Object\n",
            "0      NaN                   NaN                                NaN\n",
            "1    GPT-4     is licensed under  a proprietary agreement by OpenAI\n",
            "2     BERT    was released under                 Apache License 2.0\n",
            "3  RoBERTa    is available under                 Apache License 2.0\n",
            "4    XLNet  is distributed under                 Apache License 2.0\n",
            "\n",
            "Cleaned Data:\n",
            "   Subject              Relation                             Object\n",
            "1    GPT-4     is licensed under  a proprietary agreement by OpenAI\n",
            "2     BERT    was released under                 Apache License 2.0\n",
            "3  RoBERTa    is available under                 Apache License 2.0\n",
            "4    XLNet  is distributed under                 Apache License 2.0\n",
            "5       T5         is subject to                 Apache License 2.0\n",
            "Cleaned LLM relationships saved to: cleaned_extracted_llm_relationships.csv\n",
            "Cleaned License relationships saved to: cleaned_extracted_license_relationships.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bb8DOlSj4FkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from pyvis.network import Network\n",
        "\n",
        "# Read the first cleaned triples dataset\n",
        "file_path1 = '/content/cleaned_extracted_llm_relationships.csv'\n",
        "triples_df1 = pd.read_csv(file_path1)\n",
        "\n",
        "# Read the second cleaned triples dataset\n",
        "file_path2 = '/content/cleaned_extracted_license_relationships.csv'\n",
        "triples_df2 = pd.read_csv(file_path2)\n",
        "\n",
        "# Triple class to maintain compatibility and consistency across different datasets\n",
        "class Triple:\n",
        "    def __init__(self, arg1, predicate, arg2):\n",
        "        self.arg1 = arg1\n",
        "        self.predicate = predicate\n",
        "        self.arg2 = arg2\n",
        "\n",
        "# Create a list of Triple objects from the first DataFrame\n",
        "triples1 = [Triple(row['Subject'], row['Relation'], row['Object']) for index, row in triples_df1.iterrows()]\n",
        "\n",
        "# Create a list of Triple objects from the second DataFrame\n",
        "triples2 = [Triple(row['Subject'], row['Relation'], row['Object']) for index, row in triples_df2.iterrows()]\n",
        "\n",
        "# Combine both triples lists into a single list\n",
        "combined_triples = triples1 + triples2\n",
        "\n",
        "# Function to generate a combined graph from the list of triples\n",
        "def to_combined_graph(triples):\n",
        "    graph = nx.Graph()  # Create an undirected graph\n",
        "    argument_dict = {}  # Dictionary to map entities to node IDs\n",
        "    node_id = 1  # Start node ID from 1\n",
        "\n",
        "    for triple in triples:\n",
        "        # Reuse existing ID for subject if it already exists in the graph\n",
        "        if triple.arg1 in argument_dict:\n",
        "            arg1_id = argument_dict[triple.arg1]\n",
        "        else:\n",
        "            arg1_id = node_id\n",
        "            argument_dict[triple.arg1] = arg1_id\n",
        "            node_id += 1\n",
        "\n",
        "        predicate_id = node_id\n",
        "        arg2_id = node_id + 1\n",
        "\n",
        "        # Add nodes and edges to the graph\n",
        "        graph.add_node(arg1_id, label=triple.arg1, type='subject', color='lightblue')\n",
        "        graph.add_node(predicate_id, label=triple.predicate, type='predicate', color='lightgreen')\n",
        "        graph.add_node(arg2_id, label=triple.arg2, type='object', color='lightcoral')\n",
        "\n",
        "        graph.add_edge(arg1_id, predicate_id)\n",
        "        graph.add_edge(predicate_id, arg2_id)\n",
        "\n",
        "        node_id += 2  # Increment the node ID by 2 for the next triple\n",
        "\n",
        "    return graph, argument_dict\n",
        "\n",
        "# Generate the combined graph and the entity-to-node ID mapping dictionary\n",
        "combined_graph, argument_dict = to_combined_graph(combined_triples)\n",
        "\n",
        "# DFS query function to find all related triples starting from a specific entity\n",
        "def dfs_query(start_entity, graph, argument_dict):\n",
        "    \"\"\"\n",
        "    Perform a DFS search starting from the given entity and return all related triples in the format:\n",
        "    Subject Predicate Object.\n",
        "\n",
        "    :param start_entity: The starting entity for the DFS search (e.g., 'Turing-NLG').\n",
        "    :param graph: The networkx graph containing all triples.\n",
        "    :param argument_dict: The dictionary mapping entities to their node IDs in the graph.\n",
        "    :return: A list of strings in the format \"Subject Predicate Object\".\n",
        "    \"\"\"\n",
        "    if start_entity not in argument_dict:\n",
        "        print(f\"Entity '{start_entity}' not found in the graph.\")\n",
        "        return []\n",
        "\n",
        "    start_node = argument_dict[start_entity]\n",
        "    results = []\n",
        "    successors = nx.dfs_tree(graph, start_node)\n",
        "\n",
        "    for node in successors:\n",
        "        if graph.nodes[node]['type'] == 'subject':\n",
        "            subject_node = node\n",
        "            for predicate_node in graph.neighbors(subject_node):\n",
        "                if graph.nodes[predicate_node]['type'] == 'predicate':\n",
        "                    for object_node in graph.neighbors(predicate_node):\n",
        "                        if graph.nodes[object_node]['type'] == 'object':\n",
        "                            subject = graph.nodes[subject_node]['label']\n",
        "                            predicate = graph.nodes[predicate_node]['label']\n",
        "                            obj = graph.nodes[object_node]['label']\n",
        "                            results.append(f\"{subject}\\t{predicate}\\t{obj}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to visualize the graph using PyVis\n",
        "def display_graph(graph, output_file='my_combined_graph.html'):\n",
        "    net = Network(notebook=True)\n",
        "    for node in graph.nodes(data=True):\n",
        "        net.add_node(node[0], label=node[1]['label'], title=node[1]['label'], color=node[1]['color'])\n",
        "    for edge in graph.edges():\n",
        "        net.add_edge(edge[0], edge[1])\n",
        "    net.show(output_file)\n",
        "\n",
        "# Visualize the combined graph\n",
        "display_graph(combined_graph)\n",
        "\n",
        "# Example usage: Query for 'Turing - NLG'\n",
        "results = dfs_query('Turing - NLG', combined_graph, argument_dict)\n",
        "for result in results:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz-0lx3ToaoF",
        "outputId": "5cd53ee0-ae01-453c-ddc2-891e94705dd1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.4)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.2.2)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi, pyvis\n",
            "Successfully installed jedi-0.19.1 pyvis-0.3.2\n",
            "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
            "my_combined_graph.html\n",
            "Turing - NLG\tis applied in\tConversational Agents\n",
            "Turing - NLG\tsupports\tCreative Writing\n",
            "Turing - NLG\tis licensed under\ta proprietary agreement with Microsoft\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQnTGMCY5Ipi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ksFEQERC6Fn0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}